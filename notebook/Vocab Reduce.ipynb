{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from IPython.display import clear_output\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from multiprocessing import Pool\n",
    "import dowork\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'slim_model_v8.bin.gz'\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(filename, unicode_errors = 'replace', binary = 'True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_list = model.index2word\n",
    "to_remove = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, word in enumerate(complete_list[2064136:]):\n",
    "# #    print(index)\n",
    "# #     if \"_\" in word:\n",
    "# #         to_remove.append(word)\n",
    "# #         continue\n",
    "#     if word in stopwords_list:\n",
    "#         to_remove.append(word)\n",
    "#         continue\n",
    "#     if re.sub('[^A-Za-z0-9 ]+', '', word) != word:\n",
    "#         to_remove.append(word)\n",
    "#         continue\n",
    "#     if (word.lower()!=word) & (word.lower() in complete_list):\n",
    "#         to_remove.append(word)\n",
    "#         continue\n",
    "#     if (wordnet_lemmatizer.lemmatize(word)!=word) & (wordnet_lemmatizer.lemmatize(word) in complete_list):\n",
    "#         to_remove.append(word)\n",
    "#         continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doWork(complete_list, start_index, end_index):\n",
    "    remove_local = []\n",
    "    for index, word in enumerate(complete_list[start_index:end_index]):\n",
    "        print(index)\n",
    "        if word in stopwords_list:\n",
    "            remove_local.append(word)\n",
    "            continue\n",
    "        if re.sub('[^A-Za-z0-9 ]+', '', word) != word:\n",
    "            remove_local.append(word)\n",
    "            continue\n",
    "        if (word.lower()!=word) & (word.lower() in complete_list):\n",
    "            remove_local.append(word)\n",
    "            continue\n",
    "        if (wordnet_lemmatizer.lemmatize(word)!=word) & (wordnet_lemmatizer.lemmatize(word) in complete_list):\n",
    "            remove_local.append(word)\n",
    "            continue \n",
    "    return remove_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = Pool(processes=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pool.starmap(dowork.doWork, [(complete_list,0,375000),(complete_list,375000,750000),(complete_list,750000,1500000),(complete_list,1500000,1875000),(complete_list,1875000,2250000),(complete_list,2250000,2625000),(complete_list,2625000,3000000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "to_remove = []\n",
    "for i in range(len(results)):\n",
    "    print(i)\n",
    "    to_remove = to_remove+results[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in set(to_remove):\n",
    "    del model.vocab[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove_set = set(to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_delete = []\n",
    "for index,w in enumerate(complete_list):\n",
    "    if w in to_remove_set:\n",
    "        index_to_delete.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "model.syn0 = np.delete(model.syn0, index_to_delete, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "for w in model.vocab:\n",
    "    model.vocab[w].index = j\n",
    "    j = j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_word2vec_format(\"slim_model.bin.gz\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_list = model.index2word\n",
    "to_remove = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasNumbers(inputString):\n",
    "    return any(char.isdigit() for char in inputString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hasNumbers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-fda518be72f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mto_remove\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhasNumbers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hasNumbers' is not defined"
     ]
    }
   ],
   "source": [
    "k = 0\n",
    "to_remove = []\n",
    "for w in model.vocab:\n",
    "    if (len(w)<3) or (hasNumbers(w)):\n",
    "        print(w)\n",
    "        k = k+1\n",
    "        to_remove.append(w)\n",
    "#     if hasNumbers(w):\n",
    "#         print(w)\n",
    "#         k = k+1\n",
    "#         to_remove.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in set(to_remove):\n",
    "    del model.vocab[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove_set = set(to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_delete = []\n",
    "for index,w in enumerate(complete_list):\n",
    "    if w in to_remove_set:\n",
    "        index_to_delete.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "model.syn0 = np.delete(model.syn0, index_to_delete, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "for w in model.vocab:\n",
    "    model.vocab[w].index = j\n",
    "    j = j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model.save_word2vec_format(\"slim_model_v3.bin.gz\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "complete_list = model.index2word\n",
    "to_remove = []\n",
    "for w in model.vocab:\n",
    "    if w.isalpha():\n",
    "        continue\n",
    "    k = k+1\n",
    "    to_remove.append(w)\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in set(to_remove):\n",
    "    del model.vocab[w]\n",
    "\n",
    "to_remove_set = set(to_remove)\n",
    "\n",
    "index_to_delete = []\n",
    "for index,w in enumerate(complete_list):\n",
    "    if w in to_remove_set:\n",
    "        index_to_delete.append(index)\n",
    "\n",
    "model.syn0 = np.delete(model.syn0, index_to_delete, axis=0)\n",
    "\n",
    "j = 0\n",
    "for w in model.vocab:\n",
    "    model.vocab[w].index = j\n",
    "    j = j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_list = model.index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = []\n",
    "for w in complete_list:\n",
    "    if ((WordNetLemmatizer().lemmatize(w,'v')!=w) & (WordNetLemmatizer().lemmatize(w,'v') in complete_list)):\n",
    "        to_remove.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in set(to_remove):\n",
    "    del model.vocab[w]\n",
    "    \n",
    "to_remove_set = set(to_remove)\n",
    "\n",
    "index_to_delete = []\n",
    "for index,w in enumerate(complete_list):\n",
    "    if w in to_remove_set:\n",
    "        index_to_delete.append(index)\n",
    "        \n",
    "model.syn0 = np.delete(model.syn0, index_to_delete, axis=0)\n",
    "\n",
    "j = 0\n",
    "for w in model.vocab:\n",
    "    model.vocab[w].index = j\n",
    "    j = j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_word2vec_format(\"slim_model_v5.bin.gz\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': set(), 'n': set(), 'r': set(), 'v': set()}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from word_forms.word_forms import get_word_forms\n",
    "ret = get_word_forms(\"ministeries\")\n",
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(ret['v']) - set(ret['a'])-set(ret['n'])-set(ret['r'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_list = model.index2word\n",
    "to_remove = []\n",
    "for w in complete_list:\n",
    "    ret = get_word_forms(w)\n",
    "    to_remove = to_remove + list(set(ret['v']) - set(ret['a'])-set(ret['n'])-set(ret['r']))\n",
    "    \n",
    "to_remove = list(set(complete_list).intersection(set(to_remove)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in set(to_remove):\n",
    "    del model.vocab[w]\n",
    "    \n",
    "to_remove_set = set(to_remove)\n",
    "\n",
    "index_to_delete = []\n",
    "for index,w in enumerate(complete_list):\n",
    "    if w in to_remove_set:\n",
    "        index_to_delete.append(index)\n",
    "        \n",
    "model.syn0 = np.delete(model.syn0, index_to_delete, axis=0)\n",
    "\n",
    "j = 0\n",
    "for w in model.vocab:\n",
    "    model.vocab[w].index = j\n",
    "    j = j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_word2vec_format(\"slim_model_v6.bin.gz\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_list = model.index2word\n",
    "to_remove = []\n",
    "for w in complete_list:\n",
    "    if (w.lower()!=w) & (w.lower() in complete_list):\n",
    "        to_remove.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in set(to_remove):\n",
    "    del model.vocab[w]\n",
    "    \n",
    "to_remove_set = set(to_remove)\n",
    "\n",
    "index_to_delete = []\n",
    "for index,w in enumerate(complete_list):\n",
    "    if w in to_remove_set:\n",
    "        index_to_delete.append(index)\n",
    "        \n",
    "model.syn0 = np.delete(model.syn0, index_to_delete, axis=0)\n",
    "\n",
    "j = 0\n",
    "for w in model.vocab:\n",
    "    model.vocab[w].index = j\n",
    "    j = j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_word2vec_format(\"slim_model_v7.bin.gz\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gzip\n",
    "import os\n",
    "\n",
    "words = set()\n",
    "for dict_filename in os.listdir('dicts'):\n",
    "    with gzip.open('dicts/'+dict_filename, 'rt', encoding='utf8') as f:\n",
    "        temp = f.readlines()\n",
    "        save_len = len(temp)\n",
    "        for i in range(len(temp)):\n",
    "            temp[i] = temp[i].strip().lower()\n",
    "        temp = set(temp)\n",
    "        print('%s: %d -> %d' % (dict_filename, save_len, len(temp)))\n",
    "    words |= temp\n",
    "print('combined: %d' % (len(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_list = model.index2word\n",
    "to_remove = []\n",
    "to_remove_plural = []\n",
    "to_remove_non_english = []\n",
    "for w in complete_list:\n",
    "    if ((WordNetLemmatizer().lemmatize(w,'a')!=w) & (WordNetLemmatizer().lemmatize(w,'a') in complete_list)):\n",
    "        to_remove.append(w)\n",
    "    if w.lower() not in words:\n",
    "        to_remove_non_english.append(w)\n",
    "    if ((w[-1]=='s') & (w[:-1] in complete_list)):\n",
    "        to_remove_plural.append(w)\n",
    "    if ((w[-1]=='S') & (w[:-1] in complete_list)):\n",
    "        to_remove_plural.append(w)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = list(set(to_remove+to_remove_plural+to_remove_non_english))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in set(to_remove):\n",
    "    del model.vocab[w]\n",
    "    \n",
    "to_remove_set = set(to_remove)\n",
    "\n",
    "index_to_delete = []\n",
    "for index,w in enumerate(complete_list):\n",
    "    if w in to_remove_set:\n",
    "        index_to_delete.append(index)\n",
    "        \n",
    "model.syn0 = np.delete(model.syn0, index_to_delete, axis=0)\n",
    "\n",
    "j = 0\n",
    "for w in model.vocab:\n",
    "    model.vocab[w].index = j\n",
    "    j = j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_word2vec_format(\"slim_model_v8.bin.gz\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: The Following Error occured: list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ys16219\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\ys16219\\AppData\\Local\\Continuum\\anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PyDictionary import PyDictionary\n",
    "dictionary = PyDictionary()\n",
    "dictionary.meaning('tommy') == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete_list = model.index2word\n",
    "# to_remove = []\n",
    "# for w in complete_list:\n",
    "#     if dictionary.meaning(w) == None:\n",
    "#         print(w)\n",
    "#         to_remove.append(w)\n",
    "\n",
    "complete_list = model.index2word\n",
    "to_remove = []\n",
    "for w in complete_list:\n",
    "    if w not in wn.words():\n",
    "        to_remove.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in set(to_remove):\n",
    "    del model.vocab[w]\n",
    "    \n",
    "to_remove_set = set(to_remove)\n",
    "\n",
    "index_to_delete = []\n",
    "for index,w in enumerate(complete_list):\n",
    "    if w in to_remove_set:\n",
    "        index_to_delete.append(index)\n",
    "        \n",
    "model.syn0 = np.delete(model.syn0, index_to_delete, axis=0)\n",
    "\n",
    "j = 0\n",
    "for w in model.vocab:\n",
    "    model.vocab[w].index = j\n",
    "    j = j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_word2vec_format(\"slim_model_v9.bin.gz\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ys16219\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\ys16219\\AppData\\Local\\Continuum\\anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Noun': ['goods (or wreckage']}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.meaning('lagan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
